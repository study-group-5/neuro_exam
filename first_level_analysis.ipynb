{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study group 5's first level analysis\n",
    "\n",
    "The chunks are copied from the 13_nilearn_faceWord_stats.ipynb tutorial \n",
    "\n",
    "### In this tutorial, we will:\n",
    "\n",
    "1. Download an fMRI `BIDS` dataset with two conditions to contrast (word and face trials).\n",
    "2. Extract first level (single subject) model objects automatically from the `BIDS` dataset.\n",
    "3. Investigate the model and plot it design matrix, contrasts and effects.\n",
    "\n",
    "### What should be done next (in another notebook) - second level analysis\n",
    "1. Apply a mask (ROI); look here https://lukas-snoek.com/NI-edu/fMRI-introduction/week_6/ROI_analysis.html \n",
    "2. Train classifier\n",
    "3. Look at accuracies and the activation within the ROI\n",
    "4. Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some functionality\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "print('Starting cell:',now.strftime(\"%H:%M:%S\"))\n",
    "import os\n",
    "import pip\n",
    "os.system('python3 -m pip install numpy')\n",
    "os.system('python3 -m pip install matplotlib')\n",
    "os.system('python3 -m pip install scipy')\n",
    "os.system('python3 -m pip install panda')\n",
    "os.system('python3 -m pip install nilearn')\n",
    "os.system('python3 -m pip install pickle')\n",
    "os.system('python3 -m pip install atlasreader')\n",
    "\n",
    "#import os.path as op\n",
    "#import numpy as np\n",
    "#from numpy.linalg import inv\n",
    "#import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy.ndimage import gaussian_filter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch faceWord BIDS dataset\n",
    "The faceWord dataset is located in the `fMRI_data_raw` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(startpath):\n",
    "    \"\"\" Simple function to show directory tree. \n",
    "    From: https://stackoverflow.com/questions/9727673/list-directory-tree-structure-in-python. \"\"\"\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in sorted(files):\n",
    "            print('{}{}'.format(subindent, f))\n",
    "            \n",
    "data_dir='/work/82777/BIDS/' \n",
    "derivatives_dir=  '/work/82777/BIDS/derivatives/'          \n",
    "list_files(derivatives_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the BIDS folder?\n",
    "This `BIDS` dataset folder contains different mandatory elements ([http://bids.neuroimaging.io](http://bids.neuroimaging.io/)). These include:\n",
    "- A folder for each participant, e.g. `sub-01`. This contains a subfolder called `func` where the raw BOLD data is usually stored along with a `.tsv` file describing the events (experimental design) of the particular scan. In this dataset the raw data has been left out. Also the `anat`folder, which normally contains anatomical scans have been left out.\n",
    "- A folder called `derivatives` which is used to store processed data. In this dataset, we will can find preprocessed data files `preproc_bold.nii` and the `confounds_regressors.tsv` files (which contains the motion parameters from the spatial realignment. \n",
    "- A number of `.json` files with meta-data.\n",
    "\n",
    "NB. All the filenames contain a description of the particular experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain automatically FirstLevelModel objects and fit arguments\n",
    "From the dataset directory we automatically obtain the FirstLevelModel objects\n",
    "with their subject_id filled from the :term:`BIDS` dataset. Moreover, we obtain\n",
    "for each model a dictionary with run_imgs, events and confounder regressors\n",
    "since in this case a confounds.tsv file is available in the :term:`BIDS` dataset.\n",
    "To get the first level models we only have to specify the dataset directory\n",
    "and the task_label (`languagelocalizer`) as specified in the file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "\n",
    "#BIDS directory\n",
    "data_dir='/work/82777/BIDS/'\n",
    "# BIDS derivatives (contains preprocessed data)\n",
    "derivatives_dir='/work/82777/BIDS/derivatives' \n",
    "# Name for experiment in the BIDS directory\n",
    "task_label = 'EPIsequencewords'\n",
    "# Label for data that are spatially aligned to the MNI152 template (i.e. spatially normalised)\n",
    "space_label ='MNI152NLin2009cAsym'\n",
    "#Run the function that can gather all the needed info from a BIDS folder\n",
    "models, models_run_imgs, models_events, models_confounds = \\\n",
    "    first_level_from_bids(\n",
    "        data_dir, task_label, derivatives_folder=derivatives_dir, n_jobs=6, verbose=1,\n",
    "        img_filters=[('desc', 'preproc')])\n",
    "\n",
    "#Print the data from the first participant as sanity check\n",
    "print(models_run_imgs[0])\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "![confused_emoji.jpeg](attachment:confused_emoji.jpeg)\n",
    "\n",
    "The above code looks somewhat opaque. What is happening here? Let's look into the `first_level_from_bids` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does the `first_level_from_bid` function take as arguments?\n",
    "import inspect\n",
    "import pandas as pd\n",
    "#Get the argument from the function\n",
    "args=inspect.getfullargspec(first_level_from_bids)\n",
    "\n",
    "#Make a dataframe to display arguments (skip the first two ('dataset_path' and 'task_label') that have no defaults\n",
    "df = pd.DataFrame(args.defaults,args.args[2:])\n",
    "print(df)\n",
    "\n",
    "#Same info can also be obtained by writing first_level_from_bids?\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the function uses information about `t_r` (TR), `HRF_model` (hemodynamic response function),``drift_model`` and `high_pass` (both used for high-pass filtering), `noise_model`(used for modelling auto correlation). In other words, this function seems to implement all the elements, we have been going through in previous tutorials.\n",
    "\n",
    "<b>Nice stuff!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick sanity check on fit arguments\n",
    "Additional checks or information extraction from pre-processed data can\n",
    "be made here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, we have seen from the folder inspection, we only have one run_img per subject.\n",
    "\n",
    "Sigrid's comment; how do we see that.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Get the file names for subj01 functional data\n",
    "func01=([os.path.basename(run) for run in models_run_imgs[12]])\n",
    "print(func01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load this file and inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "\n",
    "#Get the full path\n",
    "func01_path=([os.path.abspath(run) for run in models_run_imgs[0]])\n",
    "#Load the image and inspect data size\n",
    "func_img = image.load_img(func01_path)\n",
    "print(\"Shape of functional MRI image: %s\" % (func_img.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the data contains 6x600 = 3600 volumes with a 78 x 92 x65 voxel resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the model is a `FirstLevelModel` object ([See here for details](https://nilearn.github.io/dev/modules/generated/nilearn.glm.first_level.FirstLevelModel.html)). This means that it incorporates all the same default values for setting up and estimating GLM-fMRI models as in the `first_level_from_bids`, unless else is mentioned. \n",
    "\n",
    "Here, we see the additional information that data were obtained with a TR of 1.0 seconds.\n",
    "\n",
    "fmriprep exports a large number of [confounding parameters](https://fmriprep.org/en/stable/outputs.html#confounds) which can be used to regress out noise in the data. These are saved in a .tsv file in the derivatives folder (func) with the name `sub-00xx_task-EPIsequencewords_run-1_desc-confounds_timeseries.tsv`.\n",
    "\n",
    "These are automatically imported into the `model_confounds` variable.\n",
    "Let's inspect what we find in the `models_confounds` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model confounds for first participant, first run\n",
    "print(models_confounds[0][0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are literally hundreds of confounds stored.\n",
    "Read more about them [here](https://fmriprep.org/en/stable/outputs.html#confounds)\n",
    "\n",
    "Read more about how they can be used in [this article](https://www.sciencedirect.com/science/article/pii/S1053811917310972?via%3Dihub)\n",
    "Here, we will pick the Friston24 confound variables. These correspond to the rotations and translations, we covered in the <tt>08_fMRI_preprocessing2_spatial.ipyn</tt> tutorial in combination with their derivatives and their squares. We will combine them with four measures of the global signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confound_friston24 = ['trans_x','trans_y','trans_z',\n",
    "                 'rot_x','rot_y','rot_z','trans_x_derivative1','trans_y_derivative1','trans_z_derivative1',\n",
    "                 'rot_x_derivative1','rot_y_derivative1','rot_z_derivative1','trans_x_power2','trans_y_power2','trans_z_power2',\n",
    "                 'rot_x_power2','rot_y_power2','rot_z_power2']\n",
    "\n",
    "confound_friston24_GSR = ['global_signal','global_signal_derivative1','global_signal_power2','global_signal_derivative1_power2','trans_x','trans_y','trans_z',\n",
    "                 'rot_x','rot_y','rot_z','trans_x_derivative1','trans_y_derivative1','trans_z_derivative1',\n",
    "                 'rot_x_derivative1','rot_y_derivative1','rot_z_derivative1','trans_x_power2','trans_y_power2','trans_z_power2',\n",
    "                 'rot_x_power2','rot_y_power2','rot_z_power2']\n",
    "\n",
    "# Subset confounds with selection\n",
    "for ii in range(len(models_confounds)):\n",
    "    confounds1=models_confounds[ii][:].copy()\n",
    "    for i in range(len(confounds1)):\n",
    "        confounds2=confounds1[i].copy()\n",
    "        confounds2=confounds2[confound_friston24_GSR]\n",
    "        #Removing NAs in the first row.\n",
    "        confounds2.loc[0,:]=confounds2.loc[1,:]\n",
    "        confounds1[i]=confounds2\n",
    "    models_confounds[ii][:]=confounds1\n",
    "\n",
    "\n",
    "#Print new confounds, for first participant, first run\n",
    "print(models_confounds[0][0].columns)\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events from the experiment\n",
    "Next, we will see what is in the `models_events` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_events[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will trim this a bit as well, to avoid some annoying error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model confounds for first participant, first run\n",
    "print(models_events[0][0].columns)\n",
    "\n",
    "events_sub= ['onset','duration','trial_type']\n",
    "\n",
    "\n",
    "\n",
    "# Subset confounds with selection\n",
    "for ii in range(len(models_events)):\n",
    "    events1=models_events[ii][:]\n",
    "    for i in range(len(events1)):\n",
    "        events2=events1[i]\n",
    "        events2=events2[events_sub]\n",
    "        events1[i]=events2\n",
    "    models_events[ii][:]=events1\n",
    "\n",
    "\n",
    "#Print new confounds, for first participant, first run\n",
    "print(models_events[0][0].columns)\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see check how many trials were in each condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first subject, first session\n",
    "print(models_events[0][0]['trial_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 trials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level (single subject) model estimation\n",
    "Now, that we have seen that the `first_level_from_bids` function gathers everything we need for analysis, we can get going with the analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing one participant\n",
    "\n",
    "We will start by analysing the data from the first participant and generating a report of the process and the results.\n",
    "\n",
    "If we have a [`FirstLevelModel`](https://nilearn.github.io/dev/modules/generated/nilearn.glm.first_level.FirstLevelModel.html) class object as we have created above, we can use the variable.fit() function to pair the `FirstLevelModel`object with the missing information (e.g. functional images, events and a matrix of confounding variables), assemble the model and estimate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/work/SigridAgersnapBomNielsen#1365/MR_data_analysis/MR_tutorials/first_level_analysis.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://app-293098-0.cloud.sdu.dk/work/SigridAgersnapBomNielsen%231365/MR_data_analysis/MR_tutorials/first_level_analysis.ipynb#ch0000033vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Get data and model info for 1st participant\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-293098-0.cloud.sdu.dk/work/SigridAgersnapBomNielsen%231365/MR_data_analysis/MR_tutorials/first_level_analysis.ipynb#ch0000033vscode-remote?line=1'>2</a>\u001b[0m model1\u001b[39m=\u001b[39mmodels[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://app-293098-0.cloud.sdu.dk/work/SigridAgersnapBomNielsen%231365/MR_data_analysis/MR_tutorials/first_level_analysis.ipynb#ch0000033vscode-remote?line=2'>3</a>\u001b[0m imgs1\u001b[39m=\u001b[39mmodels_run_imgs[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://app-293098-0.cloud.sdu.dk/work/SigridAgersnapBomNielsen%231365/MR_data_analysis/MR_tutorials/first_level_analysis.ipynb#ch0000033vscode-remote?line=3'>4</a>\u001b[0m events1\u001b[39m=\u001b[39mmodels_events[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get data and model info for 1st participant\n",
    "model1=models[0]\n",
    "imgs1=models_run_imgs[0]\n",
    "events1=models_events[0]\n",
    "confounds1=models_confounds[0]\n",
    "\n",
    "#Fit the model\n",
    "model1.fit(imgs1,events1,confounds1)\n",
    "#model1.zmap=model1.compute_contrast('image_neg-image_pos')\n",
    "model1.zmap=model1.compute_contrast('word_pos+word_neg+word_neu-image_neg-image_pos')\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the design matrix\n",
    "We will then use the `plot_design_matrix`function to give the model a sanity check. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "# View the design matrix\n",
    "design_matrix = model1.design_matrices_[0]\n",
    "\n",
    "\n",
    "#Plot the design matrix\n",
    "plot_design_matrix(design_matrix)\n",
    "plt.show()\n",
    "\n",
    "# Also plot time series\n",
    "plt.plot(design_matrix['image_pos'])\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Response from image_pos condition')\n",
    "plt.show()\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the design matrix contains 2 columns for the conditions + 6 for the motion parameters + 6 cosine functions for high-pass filtering + one constant/intercept.\n",
    "\n",
    "When plotting the expected response function, we see that it looks like the hemodynamic response function.\n",
    "\n",
    "\n",
    "### Defining contrasts\n",
    "\n",
    "\n",
    "Next, we need to specify the `contrasts` that we are interested in. These can be defined using using vectors or the names of of the conditions, specified in the events dataframe. Sum, subtraction and scalar multiplication are allowed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_contrast_matrix\n",
    "\n",
    "#Define contrasts\n",
    "#contrasts=['image_pos','image_neg','image_pos+image_neg','image_neg-image_pos']\n",
    "contrasts = ['word_pos+word_neg+word_neu-image_pos-image_neg']\n",
    "\n",
    "# Plot them to see what they look like\n",
    "for contrast in contrasts:\n",
    "    plot_contrast_matrix(contrast, design_matrix=design_matrix)\n",
    "    \n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the significance threshold\n",
    "To display the results, we will set a statistical threshold. Conventionally p<0.001, uncorrected for multiple comparisons has been used. \n",
    "\n",
    "It often gives a good picture of what is at play in the data, but should be interpreted with caution, due to the fact that it is uncorrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "p001_unc = norm.isf(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "Now, we will plot the results from the different contrasts. We will do so using the mean of the functional data as background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.reporting import get_clusters_table\n",
    "from nilearn.image import mean_img\n",
    "\n",
    "\n",
    "# Make a mean image for display purposes\n",
    "mean_img_ = mean_img(imgs1)\n",
    "\n",
    "for contrast in contrasts:\n",
    "    z_map = model1.compute_contrast(contrast, output_type='z_score')\n",
    "\n",
    "\n",
    "    titlex=''.join([contrast, ' ', '(p<0.001, uncor.)'])\n",
    "    plot_stat_map(z_map, bg_img=mean_img_, threshold=p001_unc,\n",
    "              display_mode='z',  black_bg=True,\n",
    "              title=titlex)\n",
    "    plt.show()\n",
    "    \n",
    "    # Make a table of cluster coordinates.\n",
    "    table = get_clusters_table(z_map, stat_threshold=p001_unc,\n",
    "                           cluster_threshold=20)\n",
    "    print(table)\n",
    "    \n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a report of the results\n",
    "\n",
    "We can also use the [`make_glm_report`](https://nistats.github.io/modules/generated/nistats.reporting.make_glm_report.html) function to make a report in one go. This report is generated as a HTML document, which can only be viewed in a notebook.\n",
    "\n",
    "This also includes a table of cluster coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import make_glm_report\n",
    "#Generate a report\n",
    "report = make_glm_report(model1,\n",
    "                         contrasts=contrasts,\n",
    "                         threshold=p001_unc,\n",
    "                         bg_img=mean_img_,\n",
    "                         )\n",
    "#Display report in HTML format\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an F contrast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create a contrast matrix for the F-contrast, spanning the two first columns in the design matrix\n",
    "effects_of_interest =np.identity(design_matrix.shape[1])[[0,1,2,3,4],:]\n",
    "\n",
    "#Let's see what it looks like\n",
    "plot_contrast_matrix(effects_of_interest, design_matrix)\n",
    "plt.show()\n",
    "\n",
    "#Similar to above, we will compute the effect\n",
    "z_map = model1.compute_contrast(effects_of_interest,\n",
    "                                  output_type='z_score')\n",
    "\n",
    "#And plot the results using an overlay\n",
    "plot_stat_map(z_map, bg_img=mean_img_, threshold=p001_unc, cmap='autumn',\n",
    "              display_mode='z',  black_bg=True,cut_coords=[-30,-20,-10,0,10,20,30],\n",
    "              title='F-contrast: (p<0.001, uncor.)')\n",
    "plt.show()\n",
    "    \n",
    "# Make a table of cluster coordinates.\n",
    "table = get_clusters_table(z_map, stat_threshold=p001_unc,\n",
    "                           cluster_threshold=20)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing all single participants in one go and plotting them\n",
    "\n",
    "Now, we will make a loop which fits all the models and plots the results. We will focus on the `image_neg-image_pos` contrast.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "now = datetime.now()\n",
    "print('Starting cell:',now.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 20)) \n",
    "model_and_args = zip(models, models_run_imgs, models_events, models_confounds)\n",
    "for m_idx, (model, imgs, events, confounds) in enumerate(model_and_args):\n",
    "    # fit the GLM, recall that the model variable is a FirstLevelModel object\n",
    "    model.fit(imgs, events, confounds)\n",
    "    # compute the contrast of interest and make a map of z-values\n",
    "    model.zmap = model.compute_contrast(contrasts)\n",
    "    #Plot each thresholded analysis\n",
    "    plotting.plot_glass_brain(model.zmap, cmap='jet',colorbar=True, threshold=p001_unc,\n",
    "                              title=('sub-' + model.subject_label),\n",
    "                              axes=axes[int(m_idx / 5), int(m_idx % 5)],\n",
    "                              plot_abs=False, display_mode='x')\n",
    "\n",
    "   \n",
    "fig.suptitle('subjects z_map image diff (unc p<0.001)')\n",
    "plotting.show()\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same but different\n",
    "Note that the single subject results look very different, although one can also see patterns in most participants that look similar.\n",
    "\n",
    "## Saving results.\n",
    "\n",
    "The analysis took a bit of time. Let's make sure to save the results, so that we don't have to run that again. \n",
    "\n",
    "#### Remember to change the directory to somewhere convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the first level models\n",
    "\n",
    "# Saving the objects:\n",
    "#f = open('/work/MikkelWallentin#6287/WordFace_first_level_models.pkl', 'wb')\n",
    "f = open('/work/SigridAgersnapBomNielsen#1365/MR_data_analysis/fMRI_data_analyzed/WordFace_first_level_our_contrast.pkl', 'wb')\n",
    "pickle.dump([models, models_run_imgs, models_events, models_confounds], f)\n",
    "f.close()\n",
    "\n",
    "# Getting back the objects:\n",
    "#f = open('store.pckl', 'rb')\n",
    "#models, models_run_imgs, models_events, models_confounds = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "now = datetime.now()\n",
    "print('Finishing cell:',now.strftime(\"%H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
